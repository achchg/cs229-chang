{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "954d3893",
   "metadata": {},
   "source": [
    "## MedMnist Dataset\n",
    "\n",
    "Followed example notebook from MedMNIST here:\n",
    "https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f2eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import autograd, optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "cudnn.benchmark = True\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0425d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238dee60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MedMNIST v2.0.2 @ https://github.com/MedMNIST/MedMNIST/\n"
     ]
    }
   ],
   "source": [
    "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cd4a6e",
   "metadata": {},
   "source": [
    "Check what are in the INFO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66facae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(INFO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601c0555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pathmnist', 'chestmnist', 'dermamnist', 'octmnist', 'pneumoniamnist', 'retinamnist', 'breastmnist', 'bloodmnist', 'tissuemnist', 'organamnist', 'organcmnist', 'organsmnist', 'organmnist3d', 'nodulemnist3d', 'adrenalmnist3d', 'fracturemnist3d', 'vesselmnist3d', 'synapsemnist3d']) 18\n"
     ]
    }
   ],
   "source": [
    "print(INFO.keys(), len(INFO.keys()))\n",
    "# Looks like 18 datasets available, 6 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c36d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_class': 'PathMNIST',\n",
       " 'description': 'The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.',\n",
       " 'url': 'https://zenodo.org/record/5208230/files/pathmnist.npz?download=1',\n",
       " 'MD5': 'a8b06965200029087d5bd730944a56c1',\n",
       " 'task': 'multi-class',\n",
       " 'label': {'0': 'adipose',\n",
       "  '1': 'background',\n",
       "  '2': 'debris',\n",
       "  '3': 'lymphocytes',\n",
       "  '4': 'mucus',\n",
       "  '5': 'smooth muscle',\n",
       "  '6': 'normal colon mucosa',\n",
       "  '7': 'cancer-associated stroma',\n",
       "  '8': 'colorectal adenocarcinoma epithelium'},\n",
       " 'n_channels': 3,\n",
       " 'n_samples': {'train': 89996, 'val': 10004, 'test': 7180},\n",
       " 'license': 'CC BY 4.0'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INFO['pathmnist']\n",
    "# Print out one of the dataset to observe the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f431caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flag = 'pathmnist'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 128\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff48013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/chi-hsuanchang/.medmnist/pathmnist.npz\n",
      "Using downloaded and verified file: /Users/chi-hsuanchang/.medmnist/pathmnist.npz\n",
      "Using downloaded and verified file: /Users/chi-hsuanchang/.medmnist/pathmnist.npz\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[.5], std=[.5])\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                               batch_size=BATCH_SIZE, \n",
    "                               shuffle=True)\n",
    "\n",
    "train_loader_at_eval = DataLoader(dataset=train_dataset, \n",
    "                                       batch_size=2*BATCH_SIZE, \n",
    "                                       shuffle=False)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                              batch_size=2*BATCH_SIZE, \n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48271e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chi-hsuanchang/opt/miniconda3/envs/local_gan/lib/python3.8/site-packages/medmnist/utils.py:25: FutureWarning: `multichannel` is a deprecated argument name for `montage`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
      "  montage_arr = skimage_montage(sel_img, multichannel=(n_channels == 3))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=56x56>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualization\n",
    "train_dataset.montage(length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "958f0773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def montage2d(imgs, n_channels, sel):\n",
      "\n",
      "    sel_img = imgs[sel]\n",
      "    montage_arr = skimage_montage(sel_img, multichannel=(n_channels == 3))\n",
      "    montage_img = Image.fromarray(montage_arr)\n",
      "\n",
      "    return montage_img\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(medmnist.utils.montage2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fcf68f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class MedMNIST2D(MedMNIST):\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        '''\n",
      "        return: (without transform/target_transofrm)\n",
      "            img: PIL.Image\n",
      "            target: np.array of `L` (L=1 for single-label)\n",
      "        '''\n",
      "        img, target = self.imgs[index], self.labels[index].astype(int)\n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.as_rgb:\n",
      "            img = img.convert('RGB')\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, target\n",
      "\n",
      "    def save(self, folder, postfix=\"png\", write_csv=True):\n",
      "\n",
      "        from medmnist.utils import save2d\n",
      "\n",
      "        save2d(imgs=self.imgs,\n",
      "               labels=self.labels,\n",
      "               img_folder=os.path.join(folder, self.flag),\n",
      "               split=self.split,\n",
      "               postfix=postfix,\n",
      "               csv_path=os.path.join(folder, f\"{self.flag}.csv\") if write_csv else None)\n",
      "\n",
      "    def montage(self, length=20, replace=False, save_folder=None):\n",
      "        from medmnist.utils import montage2d\n",
      "\n",
      "        n_sel = length * length\n",
      "        sel = np.random.choice(self.__len__(), size=n_sel, replace=replace)\n",
      "\n",
      "        montage_img = montage2d(imgs=self.imgs,\n",
      "                                n_channels=self.info['n_channels'],\n",
      "                                sel=sel)\n",
      "\n",
      "        if save_folder is not None:\n",
      "            if not os.path.exists(save_folder):\n",
      "                os.makedirs(save_folder)\n",
      "            montage_img.save(os.path.join(save_folder,\n",
      "                                          f\"{self.flag}_{self.split}_montage.jpg\"))\n",
      "\n",
      "        return montage_img\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(medmnist.dataset.MedMNIST2D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9496c4d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Conv2d(_ConvNd):\n",
      "    __doc__ = r\"\"\"Applies a 2D convolution over an input signal composed of several input\n",
      "    planes.\n",
      "\n",
      "    In the simplest case, the output value of the layer with input size\n",
      "    :math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n",
      "    can be precisely described as:\n",
      "\n",
      "    .. math::\n",
      "        \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
      "        \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
      "\n",
      "\n",
      "    where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n",
      "    :math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
      "    :math:`H` is a height of input planes in pixels, and :math:`W` is\n",
      "    width in pixels.\n",
      "    \"\"\" + r\"\"\"\n",
      "\n",
      "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "\n",
      "    * :attr:`stride` controls the stride for the cross-correlation, a single\n",
      "      number or a tuple.\n",
      "\n",
      "    * :attr:`padding` controls the amount of padding applied to the input. It\n",
      "      can be either a string {{'valid', 'same'}} or a tuple of ints giving the\n",
      "      amount of implicit padding applied on both sides.\n",
      "\n",
      "    * :attr:`dilation` controls the spacing between the kernel points; also\n",
      "      known as the à trous algorithm. It is harder to describe, but this `link`_\n",
      "      has a nice visualization of what :attr:`dilation` does.\n",
      "\n",
      "    {groups_note}\n",
      "\n",
      "    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
      "\n",
      "        - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
      "        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
      "          and the second `int` for the width dimension\n",
      "\n",
      "    Note:\n",
      "        {depthwise_separable_note}\n",
      "\n",
      "    Note:\n",
      "        {cudnn_reproducibility_note}\n",
      "\n",
      "    Note:\n",
      "        ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
      "        the input so the output has the shape as the input. However, this mode\n",
      "        doesn't support any stride values other than 1.\n",
      "\n",
      "    Args:\n",
      "        in_channels (int): Number of channels in the input image\n",
      "        out_channels (int): Number of channels produced by the convolution\n",
      "        kernel_size (int or tuple): Size of the convolving kernel\n",
      "        stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
      "        padding (int, tuple or str, optional): Padding added to all four sides of\n",
      "            the input. Default: 0\n",
      "        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,\n",
      "            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
      "        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
      "        groups (int, optional): Number of blocked connections from input\n",
      "            channels to output channels. Default: 1\n",
      "        bias (bool, optional): If ``True``, adds a learnable bias to the\n",
      "            output. Default: ``True``\n",
      "    \"\"\".format(**reproducibility_notes, **convolution_notes) + r\"\"\"\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n",
      "        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n",
      "\n",
      "          .. math::\n",
      "              H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
      "                        \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
      "\n",
      "          .. math::\n",
      "              W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
      "                        \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
      "\n",
      "    Attributes:\n",
      "        weight (Tensor): the learnable weights of the module of shape\n",
      "            :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n",
      "            :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n",
      "            The values of these weights are sampled from\n",
      "            :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "        bias (Tensor):   the learnable bias of the module of shape\n",
      "            (out_channels). If :attr:`bias` is ``True``,\n",
      "            then the values of these weights are\n",
      "            sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      "            :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
      "\n",
      "    Examples:\n",
      "\n",
      "        >>> # With square kernels and equal stride\n",
      "        >>> m = nn.Conv2d(16, 33, 3, stride=2)\n",
      "        >>> # non-square kernels and unequal stride and with padding\n",
      "        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
      "        >>> # non-square kernels and unequal stride and with padding and dilation\n",
      "        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
      "        >>> input = torch.randn(20, 16, 50, 100)\n",
      "        >>> output = m(input)\n",
      "\n",
      "    .. _cross-correlation:\n",
      "        https://en.wikipedia.org/wiki/Cross-correlation\n",
      "\n",
      "    .. _link:\n",
      "        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        in_channels: int,\n",
      "        out_channels: int,\n",
      "        kernel_size: _size_2_t,\n",
      "        stride: _size_2_t = 1,\n",
      "        padding: Union[str, _size_2_t] = 0,\n",
      "        dilation: _size_2_t = 1,\n",
      "        groups: int = 1,\n",
      "        bias: bool = True,\n",
      "        padding_mode: str = 'zeros',  # TODO: refine this type\n",
      "        device=None,\n",
      "        dtype=None\n",
      "    ) -> None:\n",
      "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
      "        kernel_size_ = _pair(kernel_size)\n",
      "        stride_ = _pair(stride)\n",
      "        padding_ = padding if isinstance(padding, str) else _pair(padding)\n",
      "        dilation_ = _pair(dilation)\n",
      "        super(Conv2d, self).__init__(\n",
      "            in_channels, out_channels, kernel_size_, stride_, padding_, dilation_,\n",
      "            False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n",
      "\n",
      "    def _conv_forward(self, input: Tensor, weight: Tensor, bias: Optional[Tensor]):\n",
      "        if self.padding_mode != 'zeros':\n",
      "            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n",
      "                            weight, bias, self.stride,\n",
      "                            _pair(0), self.dilation, self.groups)\n",
      "        return F.conv2d(input, weight, bias, self.stride,\n",
      "                        self.padding, self.dilation, self.groups)\n",
      "\n",
      "    def forward(self, input: Tensor) -> Tensor:\n",
      "        return self._conv_forward(input, self.weight, self.bias)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(nn.Conv2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b746223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple CNN model\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = Net(in_channels=n_channels, num_classes=n_classes)\n",
    "    \n",
    "# define loss function and optimizer\n",
    "if task == \"multi-label, binary-class\":\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c644031c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 704/704 [02:40<00:00,  4.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 704/704 [02:38<00:00,  4.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 704/704 [02:39<00:00,  4.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    model.train()\n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        # forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if task == 'multi-label, binary-class':\n",
    "            targets = targets.to(torch.float32)\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            targets = targets.squeeze().long()\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "958a693b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Evaluating ...\n",
      "train  auc: 0.976  acc:0.755\n",
      "test  auc: 0.958  acc:0.653\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "\n",
    "def test(split):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([])\n",
    "    y_score = torch.tensor([])\n",
    "    \n",
    "    data_loader = train_loader_at_eval if split == 'train' else test_loader\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            if task == 'multi-label, binary-class':\n",
    "                targets = targets.to(torch.float32)\n",
    "                outputs = outputs.softmax(dim=-1)\n",
    "            else:\n",
    "                targets = targets.squeeze().long()\n",
    "                outputs = outputs.softmax(dim=-1)\n",
    "                targets = targets.float().resize_(len(targets), 1)\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs), 0)\n",
    "\n",
    "        y_true = y_true.numpy()\n",
    "        y_score = y_score.detach().numpy()\n",
    "        \n",
    "        evaluator = Evaluator(data_flag, split)\n",
    "        metrics = evaluator.evaluate(y_score)\n",
    "    \n",
    "        print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n",
    "\n",
    "        \n",
    "print('==> Evaluating ...')\n",
    "test('train')\n",
    "test('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b51c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_gan",
   "language": "python",
   "name": "local_gan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
